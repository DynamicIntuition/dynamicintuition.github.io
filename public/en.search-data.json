{"/about/":{"data":{"":"","introduction#Introduction":"The Praxis Learning Research Collective is a group of people dedicated to exploring the proposal of praxis learning (PL),\nAutomated synthesis of artifacts that improve unassisted human performance from machine representations of perfect or near-perfect game-theoretic strategy.\nWe can take a look at this informally on a part-by-part basis.\nSynthesis of artifacts. Creating data of a special form.\nUnassisted performance. The ability to make good choices without a computer.\nMachine representations. Representations bound for computer processing.\nPerfect strategy. Specification of how to achieve the best outcome.\nHence, praxis in praxis learning refers to the What of learning, not the How; in PL, a praxis is learned automatically, but learning is not happening via some other meaning of â€˜praxis.â€™ Likewise, learning refers to the overall objective of coming to an effective praxis. This may seem obvious, but it is easy to think that learning refers to some specific methodology.\nDual Perspective The process of creating things that help humans understand machine-bound strategy necessarily involves some form of automated understanding. Creating a process that carries out automated understanding is a worthy objective per se, which suggests an alternative proposal,\nAutomated analysis of the intuition within perfect game-theoretic strategy.\nThis description does not involve the synthesis of artifacts which communicate this intuition to humans. As a result, the original formulation of the proposal is â€˜product oriented,â€™ whereas this perspective attends to the core of why PL is valuable.","methods#Methods":"Much like RL has a typical scenario framing, the appropriate environment for praxis learning involves the following elements being present.\nA game $G$, with a set of action histories $H$. A specified state space $S$, which is a set. An implicit abstraction $\\alpha : H \\to S$ connecting $H$ to $S$. An optimal policy function $\\pi^* : S \\to S$ (an oracle). The ask is then to find the following.\nAn alternative state space $Sâ€™$. An efficient abstraction $\\phi : S \\to Sâ€™$. A model $\\piâ€™ : Sâ€™ \\times \\Theta \\to Sâ€™$. Optimal parameters $\\theta^* \\in \\Theta$. Note\nPlease search this article for contextualized definitions of some of these terms.\nHere, the choice of $\\piâ€™$ must make $\\theta^*$ interpretable, which can be aided by an appropriate choice of $\\phi$. Importantly, no assumption is made about the functional form of $\\pi^*$.\nModel Selection In general, there is no general method to PL, because most of it relies on model selection and design. This is because the main desideratum of PL is to have interpretability. The functional form of $\\piâ€™$ and the abstract state space $Sâ€™$ are the only degrees of freedom available to achieve this objective.\nOptimization Once selections are made, however, traditional ML techniques can be employed to find an optimal $\\theta^* \\in \\Theta$ which makes $\\piâ€™_{\\theta^*}$ most closely resemble $\\pi^*$. But in general, the availability of gradient methods is uncertain.","objective#Objective":"In the landscape of objectives, PL can be characterized as an unusual form of machine learning (ML), where instead of only modeling functions as closely as possible, we add the upfront constraint that the model must be inherently interpretable.\nArea of Concern PL also constrains the set of models under consideration to functions which approximate a game-theoretic strategy. The term policy function, which is adopted from reinfocement learning (RL), describes such functions.\nPraxis learning is not concerned with optimizing policy functions under the objective of utility maximization. There are many incredibly successful fields of research dedicated to this objective under different constraints. Instead, the optimization objective of PL is to have a model closely approximate an existing policy function with access to it.\nTherefore, while PL can be characterized as ML, its purpose is strictly translation across model families. With all this, PL could be characterized as transfer learning (TL) for game-theoretic AI interpretability. (Although this is deceiving, as the methods of TL differ.)","statements#Statements":"Mission Given that the idea of praxis learning is unexplored, the utmost priority is to first motivate, assess, and communicate its tractability via comprehensive studies. Later, focus can shift to connecting a taxonomy of topics within PL to motivate additional research and future work. This is all under the objective of AI interpretability constrained to game-theoretic settings.\nCollaboration In the spirit of a collective, we aspire for our work to be characterized by seamless reproducibility, proactive transparency, and clear exposure. This is in the interest of accelerating our mission and improving our intellectual community."},"title":"About"},"/knowledge/":{"data":{"":"ðŸ‘‹ Hello! This is the knowledge base for PLRC software projects.\nIn general, this is not documentation! We like to use language- or registry-specific media for those purposes. Everything that cannot be comfortably put there (or that could or should change faster than code) goes here.","order#Order":"We organize everything by project. There is no strict correspondence between projects and software repositories/packages. Therefore, one project could have many (or no) repositories or packages, but how each project organizes itself is very much subject to being under that projectâ€™s section.\nApart from that, we keep some meta-information such as this page. An important one is the yearly strategic plan we make for our projects, which you can find here.","projects#Projects":" Graphor Plugin-based platform for dataset generation over implicit graphs.\nNova Collection of Graphor plugins for sequential deterministic games.\nHermes Integrated system for generating text-based descriptions of strategy."},"title":"Knowledge"},"/knowledge/graphor/":{"data":{"":"Graphor is a set of software containing interfaces for representing implicit graphs through specific functional forms and utilities for computing features associated with their vertices for their downstream analysis as datasets.\nConcretely, graphor is a Rust library crate which implements a plugin system where actual implementations of implicit graphs can be dynamically linked. So, the Graphor project also maintains packages with graphor pre-baked into them that conveniently expose it in extensible ways (without re-compilation).","plugins#Plugins":"A plugin is a collection of abstractions together with an IG. Each abstraction produces one or many named features. Put simply, the objective of any graphor product is to allow users to pick among a menu of available features on a per-plugin (and therefore per-IG) basis, and then have graphor internals construct a dataset automatically.\nTo aid the process of writing useful abstractions for domain-specific plugins (such as the game-theoretic ones we are concerned with), different interfaces, algorithms, and utilities are included in the library behind compiler flags (so you only get them if you need them).\nFrom a project management standpoint, graphor as a project avoids the growth associated with more IGs through the plugin system (by offloading to dynamic libraries), but welcomes growth from plugin support (through the addition of useful definitions and algorithms).","primitives#Primitives":"There are two objects in primary focus.\nImplicit graphs (IGs). Graph representations in terms of an initial vertex $v \\in V$ and a transition function $t : V \\to \\mathcal{P}(V)$. Abstractions: Adjacency-preserving Morphisms (functions) of the form $a : V \\to Vâ€™$, from vertices in a lift $G = (V, E)$ to a possible correspondant $Gâ€™ = (Vâ€™, Eâ€™)$. Note\nThis might seem unmotivated. Please see this article for a contextualized explanation.","stories#Stories":"The graphor library crate can be useful in many ways. We will explain this through three different user stories of a fictional Joe User.\nPlugin AuthorLibrary AuthorBinary Author Situation Joe is a mathematician making a rewrite system to see how many theorems he can prove by brute-forcing formal rewrites.\nProblem Joeâ€™s project only involves one formal system with well-defined rewrite rules. He can write code, but does not have the technical know-how to manage the amount of permutations that his system will explore during exploration.\nSolution Joe finds the Graphor project, and notices that it already has an implementation of a shortest-paths algorithm. His solution is to write a graphor plugin representing his formal rewrite system.\nImplementation Joe writes Rust code implementing the right interfaces for his rewrite system. He then compiles his code as a dynamic library, allowing it to be used as a plugin.\nUsage Joe then uses the graphor-cli binary crate of the Graphor project to register his plugin and run the shortest-paths algorithm on a few different input theorems.\nJoe could have also used the graphor Python package to do the same through a Python script (and have programatic access to artifacts generated in the process).\nSituation Joe is a computer scientist implementing a toolchain for formal grammar checkers (programs that check whether or not a string belongs to a formal language) that work via recursive descent.\nProblem Recursive descent can be very expensive, and Joe does not want to spend too much time making optimizations.\nSolution Joe finds the Graphor project, and notices that it already has an implementation of a vertex-inclusion graph algorithm. His solution is to simply write a wrapper around the graphor library crate under the semantics of different types of formal grammars.\nImplementation Joe writes Rust wrapper traits (interfaces) that, once implemented, automatically give usersâ€™ items access to graphor traits. He does this via blanket implementations.\nUsage Joe then publishes his toolchain as a Rust library crate. This allows others to write Rust code that implements Joeâ€™s interface definitions, compile their code as a plugin, and use it through any graphor frontend (such as the graphor-cli binary crate).\nJoe could even write a wrapper (or extension) around the graphor CLI utility (which graphor-cli calls directly), and publish his own appropriately-themed binary with a dedicated UI. (Or write an entire user interface from scratch.)\nSituation Joe is a classical AI afficionado who likes to solve small games and publish his results.\nProblem Joe would like to spend his time working on implementing game rules, not on building infrastructure.\nSolution Through the Graphor project, Joe observes that he can represent many games he is interested in as IGs and apply well-known algorithms to solve them.\nImplementation Joe implements graphorâ€™s interfaces for a lot of games, then self-registers them in a binary program (without compiling dynamic libraries). His program includes a user interface that allows him to interact with is implementations.\nUsage Joe publishes his binary as a standalone program which comes pre-loaded with a lot of games, and accepts user commands that allow it to generate solutions to the games he coded.\nWhen he has the time, Joe will make it so that, in addition to being able to solve all of the pre-loaded games in the binary, users will be able to write graphor plugins that his binary can register and also solve. This will be possible with minimal effort due to graphorâ€™s architecture."},"title":"Graphor"},"/knowledge/graphor/organization/":{"data":{"":"Here we focus on the organization of the Graphor platform across different libraries, binaries, and modules. This is intended as an introduction to the project, not as a reference.","module-diagram#Module Diagram":"graph TD; A[graphor repository] B[graphor-traits] C[graphor-algos] D[graphor-cli] E[graphor bin] F[graphor lib] G[graphor py] H[plugin] I[graphor-core] E --\u003e |compiles| F E --\u003e |calls| D D --\u003e |calls| I E --\u003e |ingests| H H --\u003e |implements| B H --\u003e |monomorphizes| C H --\u003e |compiles| F G --\u003e |exposes| I C --\u003e |consumes| B F --\u003e |includes| B F --\u003e |includes| C F --\u003e |includes| I A --\u003e |contains| E F --\u003e |optional| D A --\u003e |contains| F A --\u003e |contains| G I --\u003e |reads / calls| H","remarks#Remarks":"Here we will attempt to justify and explain some design decisions in a Q\u0026A format.\nWhat is ____? Here is each componentâ€™s description.\ngraphor repository. The GitHub repository containing everything here. graphor lib. graphor itself. Contains all Rust code, some behind feature gates. graphor-core. Core plugin execution API in gaphor. graphor-cli. CLI implementation in graphor; minimal frontend for core API. graphor py. Python PyPi package containing bindings to the graphor-core API. graphor bin. Rust binary crate that allows registering and executing plugins. graphor-algos. Algorithm implementations in graphor. graphor-traits. Trait declarations in graphor. plugin. An example plugin implementing graphor traits. Why dynamic libraries? In principle, the amount of existing plugins is unbounded, as well as the amount of people working on one at any given point. At least, it is somewhat independent of the amount of people working on graphor. Dynamic libraries as a design decision permits independent authorship without the need for a central body of developers, who would otherwise be a development bottleneck.\nWhere does data live? When a plugin is implemented, all of the generic code used in graphor is specialized at the time of compilation. This includes the algorithms that perform graph traversals and the database code that stores the desired features associated with graph vertices.\nSo, when a plugin is â€™executed,â€™ the core routines of graphor figure out the minimum amount of algorithms that need to be executed to figure out the features requested by a user. Then they are executed, and the data they compute is stored by plugin code into a persistent database file which is later recovered and coalesced by graphor."},"title":"Organization"},"/knowledge/hermes/":{"data":{"":" Warning\nThis page is under construction."},"title":"Hermes"},"/knowledge/nova/":{"data":{"":" Warning\nThis page is under construction."},"title":"Nova"},"/knowledge/strategy/":{"data":{"":"In accordance to our research mission, projects are distributed with the objective of assessing the tractability of Praxis Learning. See our About page for a fundamental perspective on our work.","roadmap#Roadmap":" Graphor Graphor is our plugin-based platform for dataset generation from a primitive of implicit graphs, which the aforementioned games are a case of. It is designed for language interoperability and quick experimentation, with diminished emphasis on the size of graphs it can support.\nNova Nova is the post-Graphor future of GamesmanNova, an all-in-one Rust system for search in sequential games. Since Graphor will be an integrated system, Nova will simply become a collection of Graphor plugins.\nPlugins Once Graphor and Nova are compatible, contributions will be open for Nova. A series of game Graphor plugins will be added to the system together with feature definitions.\nHermes With plugins available for dataset generation, feasability studies will be conducted on the proposal of Hermes, an end-to-end system for generating textual descriptions of game-theoretic strategies.","summary#Summary":"Motivated by the profound differences across strategic situations, we attempt to maximize the number of samples available for testing our methods during experimentation.\nWe specifically seek to have standardized access to many provably correct strategies. By focusing on the deterministic case of game strategy, we restrict potential shortcomings in performance to our methods, increasing the statistical efficiency of our sample strategies.\nAs such, our projects facilitate consistent production of datasets associated with provably correct strategies in this case of games. We incorporate design decisions that accelerate this process and take on projects that address this objective.","timeline#Timeline":" Graphor and Nova are set to be compatible by mid-May.\nSignificant progress is expected on Graphor by late March.\nPreparations for accepting contributions to plugins will be done during the Summer.\nPlugin additions will be accepted during the Fall, concurrent to the development of learned strategy representations.\nA minimum viable product for Hermes is tentative for early 2026."},"title":"Strategy"},"/people/":{"data":{"":" Max FierroHowdy! "},"title":"People"},"/reports/markov_models/":{"data":{"":"","abstract#Abstract":"In praxis learning, choosing an interpretable functional form as a policy approximation model is paramount. Equally important is to ensure that it is well-defined defined over interpretable domains. Many such interpretable domains are the result of class-valued abstractions of the observable state space, as visual classification is a task that humans excel at. Motivated by this fact, we provide an interpretable functional form that is valid over multiclass spaces in the form of an $n$-gram Markov model approximation of dynamics under optimal policy.","background#Background":"$n$-gram models were developed as a rudimentary statistical model of language. By assuming an $n^{th}$-order Markov property on the probability of observing a word $w_{t + 1}$ at discrete time $t + 1$ given a history sequence $\\langle w_i \\rangle_{i \\in [1, \\, t]}$,\n\\[ \\begin{equation} P(w_1, \\, \\ldots, w_{t + 1}) = P(w_1, \\dots, w_{t - n - 1}) \\prod_{i = 0}^{n - 1} P(w_{t+1} \\mid w_{t-n}, \\dots, w_t), \\end{equation} \\]straightforward maximum likelihood estimation shows that this probability is the proportion of times that the sequence $\\langle w_{t-n}, \\, \\ldots, w_t \\rangle$ appears before $w_{t + 1}$ in observations. This is can be seen as frequentist inference, making the probability measure intuitive.\nWhen applied to a set of symbols (words) $S$, such a model implies a Markov chain over the product $S^n = S \\times \\cdots \\times S$. It follows that the chainâ€™s stochastic matrix $\\Pi$ is an element of $\\mathbb{R}^{k \\times k^n}$ with $k = |S|$, meaning that the number of learnable parameters grows exponentially with the order of the model for a fixed $S$.\nAs a result of upholding the Markov property, $n$-gram models are stationary, meaning that the probabilities they assign to a sequence are invariant with respect to shifts in the time index. This flaw makes them incompatible with natural language to any useful extent.\nRules of Thumb Many heuristics taught in strategic decision-making can be described to be conditionals over the result of classification exercises. For example, there is a rule of thumb in Chess which calls for protecting oneâ€™s own king if it is open.\nWhen implementing this heuristic, a player necessarily performs a classification task via a mapping $\\phi : S \\to \\{\\text{Yes}, \\, \\text{No}\\}$ from the set of board states to an answer to the heuristicâ€™s condition, where experience insists that if a playerâ€™s $\\phi$ is sufficiently close to ground truth, they obtain a performance improvement.\nNaturally, the complexity involved in evaluating a classification $\\phi_h(s)$ for some state $s \\in S$ should be minimal so that its heuristic $h$ can be implemented without computer assistance. In many cases, their simplicity to humans (i.e., how intuitive they are) directly translates to the simplicity of implementing them in other models of computation; it is generally easy to program such functions.\nHowever, humans can obtain an unexplainable intuitive understanding of a game. In such cases, the classification exercises they carry out for their expert heuristics are mappings onto a set of abstract characteristics (e.g., area â€˜crowdednessâ€™ in Chess). This can be seen loosely as a human version of feature learning.\nBut even in these cases, it is relatively simple to train a model which replicates a humanâ€™s capacity to perform classification for expert heuristics via human-labeled datasets. Hence, one can generally assume access to efficient classifiers for human-interpretable features.\nAbstract Strategy Given a morphism (an abstraction) $\\alpha : S \\to \\tilde{S}$ over a state set $S$, the lack of an injectivity constraint could produce a situation where for an arbitrary policy $\\pi : S \\to S$ satisfying $\\pi(s) = a$ and $\\pi(s^\\prime) = b$ with distinct $a, \\, b, \\, s, \\, s^\\prime \\in S$,\n$$ \\begin{equation} \\alpha(s) = \\alpha(s^\\prime) \\;\\; \\text{and} \\;\\; \\alpha(a) \\neq \\alpha(b). \\end{equation} $$Therefore, attempting to obtain a counterpart $\\tilde{\\pi} : \\tilde{S} \\to \\tilde{S}$ (an â€˜abstract strategyâ€™) which preserves the information in $\\pi$ is oftentimes not possible, as $\\tilde{\\pi}(\\alpha(s)) = \\tilde{\\pi}(\\alpha(s^\\prime))$ would have to â€˜rememberâ€™ the distinct $\\pi(s) = a$ and $\\pi(s^\\prime) = b$.","inference#Inference":"When at a state $s \\in S$, a human player can consider the set of next possible states $t(s)$ (where $t : S \\to \\mathcal{P}(S)$ is a transition function). Ideally, they would perform combinatorial optimization across all elements $s^\\prime \\in t(s)$ under the objective of maximizing the probability that their action is observed across all abstract state space transitions $S^{(\\alpha)}$. Essentially, this is maximum likelihood estimation.\nWhile this is possible to an extent due to the simplicity of the abstractions in consideration (which map onto small sets of classes, reducing maximization objectives during MLE), the true value of the model is in the subjective analysis of each $\\Pi^{(\\alpha)}$. Additionally, quantitative techniques (such as finding the static distribution and convergence rate of these matrices) may illustrate interpretable patterns in the dynamics of $\\pi$, depending on $\\langle \\phi^{(\\alpha)} \\rangle$.","metadata#Metadata":" @misc{fierro2025markov, author = {Fierro, Max}, title = {N-Gram Markovian Modeling of Optimal Policy Under Interpretable Abstractions}, institution = {EECS Department, University of California, Berkeley}, howpublished = {Online}, url = {http://plrcollective.github.io/reports/markov_models/}, year = {2025}, month = {February}, day = {21}, abstract = {...} } ","model#Model":"Let $\\langle \\phi^{(\\alpha)} : S \\to S^{(\\alpha)} \\rangle_{\\alpha \\in \\Alpha}$ be a collection of abstractions enumerated in $\\Alpha$, and $\\pi : S \\to S$ a policy over $S$. Observing $(2)$, we propose modeling class-conditional transition probability distributions,\n$$ \\begin{equation} P^{(\\alpha)}_{t+1}(k) = P[\\phi^{(\\alpha)}(\\pi^{t + 1}(s)) = k \\; | \\; \\phi^{(\\alpha)}(\\pi^t(s)) = k_t, \\, \\ldots, \\, \\phi^{(\\alpha)}(\\pi^0(s)) = k_0], \\end{equation} $$of the elements $k_i \\in S^{(\\alpha)}$ via an $n$-gram model. This effectively establishes sequences in the set $\\phi^{(\\alpha)}(S)$ via repeated aplication of $\\pi$ within $S$ (following the dynamics of $\\pi$), so that in the above equation $\\pi^t(s) = \\pi_t(\\pi_{t-1}(\\ldots\\pi_1(s)))$. This yields a collection of stochastic matrices $\\langle \\Pi^{(a)} \\rangle_{\\alpha \\in \\Alpha}$ with\n$$ \\Pi^{(\\alpha)}_{i, j} = P[\\, i \\text{ is observed at time } t \\; | \\; j \\text{ is observed immediately before}\\,], $$where $i \\in S^{(\\alpha)}$ and $j \\in (S^{(\\alpha)})^n$. The amount of learnable parameters (i.e., the size) of such a model $M = \\langle \\Pi^{(a)} \\rangle_{\\alpha \\in \\Alpha}$ is therefore\n$$ \\begin{equation} |M| = \\sum_{\\alpha \\in \\Alpha} |S^{(\\alpha)}|^n \\, (|S^{(\\alpha)}| - 1). \\end{equation} $$","remarks#Remarks":"Establishing an approximation of optimal policy in the form of a Markov process provides an interpretable functional representation that is able to work with intuitive abstractions. Thus, it constitutes a valid representation of a praxis.\nExplorations The following are left as potential avenues of analysis relating to the model family.\nSmoothing techniques, and an analysis of their benefit in the context of optimal policy. Non-interpretability of $n$-gram model successors; in particular transformer attention. Skip-gram models as an extension of this family. ","training#Training":"The parameter space for a model $M$ of order $n$ is precisely\n$$ \\begin{equation} \\Theta = \\large{\\times_{\\alpha \\in \\Alpha}} \\left[ \\bold{S}^{|S^{(\\alpha)}|^n} \\right]^{|S^{(\\alpha)}|}, \\end{equation} $$(where $\\bold{S}^k$ denotes the $k$-dimensional unit sphere). Finding an optimal $\\theta^* \\in \\Theta$ follows the same procedure as any $n$-gram model. Hence, we simply provide the generic closed-form solution written in terms of the objects at hand,\n$$ \\begin{equation} \\Pi^{(\\alpha)}_{i, j} = \\frac{1}{N} \\sum_{s \\in S} I^{(\\alpha)}_{i,j}(\\pi^n(s), \\langle \\pi^i(s) \\rangle_{i \\in [0, \\, n)}), \\end{equation} $$where\n$$ \\begin{equation*} I^{(\\alpha)}_{i,j}(a, \\langle b_i \\rangle_{i \\in [0, \\, n)}) = \\begin{cases} 1 \u0026 \\text{if } \\; \\phi^{(\\alpha)}(a) = i \\; \\text{ and } \\; \\phi^{(\\alpha)}(b) = j, \\\\ 0 \u0026 \\text{otherwise}, \\end{cases} \\end{equation*} $$and $N$ is the number of length-$(n + 1)$ contiguous subsequences in the dynamics of $\\pi$, which can be easily sketched while computing the sum in $(5)$.\nSources The nature of the policy operator $\\pi$ is such that there exists some $s \\in S$ wihtout an $s^\\prime$ such that $\\pi(s^\\prime) = s$. Here, $s$ is called a source within the dynamics of $\\pi$. This constitutes a problem, as the start $s_0$ of the game for which $S$ is a state space is necessarily a source (which may not be unique); therefore, an attempt to find an $n$-length sequence of moves leading up to a state less than $n$ applications of $\\pi$ away from a source in its dynamics may fail.\nThis is important because it is a step necessary to compute the $\\Pi^{(\\alpha)}_{i, j}$$^{\\text{th}}$ parameter of the model, where $i$ is the parameter that is too close to a source to have a valid $n$-gram history. A solution which does not significantly alter transition distributions of $\\Pi^{(\\alpha)}$ is to sample missing elements of $n$-gram histories from a uniform distribution while computing its entries. If this measure is taken, $N$ can be set to $|S|$ in $(5)$, avoiding the need for sketching proportions.\nSinks In many traditional definitions of a policy $\\pi$, there may exist elements $s^\\prime_i$ of $S$ over which $\\pi$ is not defined, as they are terminal in the game under representation. These are sinks in the dynamics of $\\pi$, and should never be considered as part of a history while computing model parameters."},"title":"N-Gram Markovian Modeling of Optimal Policy Under Interpretable Abstractions"}}